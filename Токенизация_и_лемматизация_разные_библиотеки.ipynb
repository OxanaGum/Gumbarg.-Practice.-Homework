{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проведение токенизации и лемматизации с помощью разных библиотек. Плюсы и минусы библиотек Наташа, Pymorphy2, UDPipe, Pymystem3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация - это процесс разбиения фразы, предложения, абзаца или всего текстового документа на более мелкие единицы, например, отдельные слова или термины. Каждое из этих меньших подразделений называется токенами.\n",
    "\n",
    "Лемматизация - это приведение всех слов к начальной форме (иначе - лемме): глаголы - в инфинитив, существительные - в И.п. ед.ч., прилигательные - в ед.ч. м.р."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Наташа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека natasha python - это набор инструментов для обработки естественного языка на языке Python.\n",
    "\n",
    "Он включает в себя ряд модулей и функций для выполнения разных задач, таких как извлечение именованных сущностей, анализ тональности и классификацию текстов. \n",
    "\n",
    "Основные преимущества библиотеки natasha python:\n",
    "\n",
    "Простой в использовании и понимании.\n",
    "Эффективный алгоритм для обработки естественного языка.\n",
    "Поддержка различных языков и интеграция с другими библиотеками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На этой странице вы можете скачать стили Mendeley и Zotero для оформления литературы по ГОСТ 7.0.100-2018 с Алфавитной сортировкой или В порядке появления ссылок. С цифровыми ссылками вида [1] или с текстовыми ссылками вида [Иванов и др., 2008; Бобров, 2018]. Стили доступны в виде файлов в формате .csl.\n"
     ]
    }
   ],
   "source": [
    "text = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import(\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "#    NewsSyntaxParser,\n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "#syntax_parser = NewsSyntaxParser(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc - это объект для построения проанализированного текста. \n",
    "# С его помощью можно, например, извлечь структурированную информацию (фамилию, имя и отчество) из текста на русском языке.\n",
    "doc = Doc(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# методы segment, tag_morph для сегментации на токены и предложения, анализа морфологии\n",
    "doc.segment(segmenter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocToken(stop=2, text='На'),\n",
       " DocToken(start=3, stop=7, text='этой'),\n",
       " DocToken(start=8, stop=16, text='странице'),\n",
       " DocToken(start=17, stop=19, text='вы'),\n",
       " DocToken(start=20, stop=26, text='можете'),\n",
       " DocToken(start=27, stop=34, text='скачать'),\n",
       " DocToken(start=35, stop=40, text='стили'),\n",
       " DocToken(start=41, stop=49, text='Mendeley'),\n",
       " DocToken(start=50, stop=51, text='и'),\n",
       " DocToken(start=52, stop=58, text='Zotero'),\n",
       " DocToken(start=59, stop=62, text='для'),\n",
       " DocToken(start=63, stop=73, text='оформления'),\n",
       " DocToken(start=74, stop=84, text='литературы'),\n",
       " DocToken(start=85, stop=87, text='по'),\n",
       " DocToken(start=88, stop=92, text='ГОСТ'),\n",
       " DocToken(start=93, stop=105, text='7.0.100-2018'),\n",
       " DocToken(start=106, stop=107, text='с'),\n",
       " DocToken(start=108, stop=118, text='Алфавитной'),\n",
       " DocToken(start=119, stop=130, text='сортировкой'),\n",
       " DocToken(start=131, stop=134, text='или'),\n",
       " DocToken(start=135, stop=136, text='В'),\n",
       " DocToken(start=137, stop=144, text='порядке'),\n",
       " DocToken(start=145, stop=154, text='появления'),\n",
       " DocToken(start=155, stop=161, text='ссылок'),\n",
       " DocToken(start=161, stop=162, text='.'),\n",
       " DocToken(start=163, stop=164, text='С'),\n",
       " DocToken(start=165, stop=174, text='цифровыми'),\n",
       " DocToken(start=175, stop=183, text='ссылками'),\n",
       " DocToken(start=184, stop=188, text='вида'),\n",
       " DocToken(start=189, stop=190, text='['),\n",
       " DocToken(start=190, stop=191, text='1'),\n",
       " DocToken(start=191, stop=192, text=']'),\n",
       " DocToken(start=193, stop=196, text='или'),\n",
       " DocToken(start=197, stop=198, text='с'),\n",
       " DocToken(start=199, stop=209, text='текстовыми'),\n",
       " DocToken(start=210, stop=218, text='ссылками'),\n",
       " DocToken(start=219, stop=223, text='вида'),\n",
       " DocToken(start=224, stop=225, text='['),\n",
       " DocToken(start=225, stop=231, text='Иванов'),\n",
       " DocToken(start=232, stop=233, text='и'),\n",
       " DocToken(start=234, stop=236, text='др'),\n",
       " DocToken(start=236, stop=237, text='.'),\n",
       " DocToken(start=237, stop=238, text=','),\n",
       " DocToken(start=239, stop=243, text='2008'),\n",
       " DocToken(start=243, stop=244, text=';'),\n",
       " DocToken(start=245, stop=251, text='Бобров'),\n",
       " DocToken(start=251, stop=252, text=','),\n",
       " DocToken(start=253, stop=257, text='2018'),\n",
       " DocToken(start=257, stop=258, text=']'),\n",
       " DocToken(start=258, stop=259, text='.'),\n",
       " DocToken(start=260, stop=265, text='Стили'),\n",
       " DocToken(start=266, stop=274, text='доступны'),\n",
       " DocToken(start=275, stop=276, text='в'),\n",
       " DocToken(start=277, stop=281, text='виде'),\n",
       " DocToken(start=282, stop=288, text='файлов'),\n",
       " DocToken(start=289, stop=290, text='в'),\n",
       " DocToken(start=291, stop=298, text='формате'),\n",
       " DocToken(start=299, stop=300, text='.'),\n",
       " DocToken(start=300, stop=303, text='csl'),\n",
       " DocToken(start=303, stop=304, text='.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.tokens[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocSent(stop=162, text='На этой странице вы можете скачать стили Mendeley..., tokens=[...]),\n",
       " DocSent(start=163, stop=244, text='С цифровыми ссылками вида [1] или с текстовыми сс..., tokens=[...]),\n",
       " DocSent(start=245, stop=259, text='Бобров, 2018].', tokens=[...]),\n",
       " DocSent(start=260, stop=304, text='Стили доступны в виде файлов в формате .csl.', tokens=[...])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'На': 'на',\n",
       " 'этой': 'этот',\n",
       " 'странице': 'страница',\n",
       " 'вы': 'вы',\n",
       " 'можете': 'мочь',\n",
       " 'скачать': 'скачать',\n",
       " 'стили': 'стиль',\n",
       " 'Mendeley': 'mendeley',\n",
       " 'и': 'и',\n",
       " 'Zotero': 'zotero',\n",
       " 'для': 'для',\n",
       " 'оформления': 'оформление',\n",
       " 'литературы': 'литература',\n",
       " 'по': 'по',\n",
       " 'ГОСТ': 'гост',\n",
       " '7.0.100-2018': '7.0.100-2018',\n",
       " 'с': 'с',\n",
       " 'Алфавитной': 'алфавитный',\n",
       " 'сортировкой': 'сортировка',\n",
       " 'или': 'или',\n",
       " 'В': 'в',\n",
       " 'порядке': 'порядок',\n",
       " 'появления': 'появление',\n",
       " 'ссылок': 'ссылка',\n",
       " '.': '.',\n",
       " 'С': 'с',\n",
       " 'цифровыми': 'цифровой',\n",
       " 'ссылками': 'ссылка',\n",
       " 'вида': 'вид',\n",
       " '[': '[',\n",
       " '1': '1',\n",
       " ']': ']',\n",
       " 'текстовыми': 'текстовый',\n",
       " 'Иванов': 'иванов',\n",
       " 'др': 'др',\n",
       " ',': ',',\n",
       " '2008': '2008',\n",
       " ';': ';',\n",
       " 'Бобров': 'бобров',\n",
       " '2018': '2018',\n",
       " 'Стили': 'стиль',\n",
       " 'доступны': 'доступный',\n",
       " 'в': 'в',\n",
       " 'виде': 'вид',\n",
       " 'файлов': 'файл',\n",
       " 'формате': 'формат',\n",
       " 'csl': 'csl'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Natasha решает задачу лемматизации, использует Pymorphy2 и результаты морфологического разбора\n",
    "# поэтому если запустить лемматизацию до морф анализа, то выйдет ошибка\n",
    "for token in doc.tokens:\n",
    "    token.lemmatize(morph_vocab)\n",
    "\n",
    "{_.text: _.lemma for _ in doc.tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = NewsEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проводим морфологический анализ doc.tag_morph(morph_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Zotero': 'Zotero', 'Иванов': 'Иванов', 'Бобров': 'Бобров'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# нормализация именованных сущностей (имена, названия организаций, топонимы): \n",
    "# привести словосочетание к нормальной форме с использованием результатов синтаксического разбора, \n",
    "# учитывая связи между словами\n",
    "\n",
    "for span in doc.spans:\n",
    "    span.normalize(morph_vocab)\n",
    "\n",
    "{_.text: _.normal for _ in doc.spans}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На этой странице вы можете скачать стили Mendeley и Zotero для \n",
      "                                                    ORG───     \n",
      "оформления литературы по ГОСТ 7.0.100-2018 с Алфавитной сортировкой \n",
      "или В порядке появления ссылок. С цифровыми ссылками вида [1] или с \n",
      "текстовыми ссылками вида [Иванов и др., 2008; Бобров, 2018]. Стили \n",
      "                          PER───              PER───               \n",
      "доступны в виде файлов в формате .csl.\n"
     ]
    }
   ],
   "source": [
    "# Извлечение именованных сущностей\n",
    "from natasha import NewsNERTagger\n",
    "\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "doc.tag_ner(ner_tagger)\n",
    "doc.ner.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pymorphy2 - морфологический анализатор для русского языка, написанный на языке Python и использующий словари из OpenCorpora. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "pm = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#from nltk import sent_tokenize, word_tokenize, regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Например, если мы хотим найти наиболее похожее слово к \"кофе\", мы можем вычислить косинусное расстояние между вектором \"кофе\" и векторами других слов в модели Word2Vec. Слово, которое имеет наименьшее косинусное расстояние, будет считаться наиболее похожим на \"кофе\".\n"
     ]
    }
   ],
   "source": [
    "text_tok = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проводим токенизацию через NLTK, так как лемматизатор Pymorphy2 рабтает с токенами\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "text_tokens = tokenizer.tokenize(text_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse(word='например', tag=OpencorporaTag('CONJ,Prnt'), normal_form='например', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'например', 2027, 0),)),\n",
      "\n",
      "Parse(word='если', tag=OpencorporaTag('CONJ'), normal_form='если', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'если', 20, 0),)),\n",
      "\n",
      "Parse(word='мы', tag=OpencorporaTag('NPRO,1per plur,nomn'), normal_form='мы', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'мы', 1990, 0),)),\n",
      "\n",
      "Parse(word='хотим', tag=OpencorporaTag('VERB,impf,tran plur,1per,pres,indc'), normal_form='хотеть', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'хотим', 2999, 2),)),\n",
      "\n",
      "Parse(word='найти', tag=OpencorporaTag('INFN,perf,tran'), normal_form='найти', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'найти', 2013, 0),)),\n",
      "\n",
      "Parse(word='наиболее', tag=OpencorporaTag('ADVB'), normal_form='наиболее', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'наиболее', 3, 0),)),\n",
      "\n",
      "Parse(word='похожее', tag=OpencorporaTag('ADJF,Qual neut,sing,nomn'), normal_form='похожий', score=0.3333333333333333, methods_stack=((<DictionaryAnalyzer>, 'похожее', 370, 14),)),\n",
      "\n",
      "Parse(word='слово', tag=OpencorporaTag('NOUN,inan,neut sing,nomn'), normal_form='слово', score=0.615384, methods_stack=((<DictionaryAnalyzer>, 'слово', 54, 0),)),\n",
      "\n",
      "Parse(word='к', tag=OpencorporaTag('PREP'), normal_form='к', score=0.998113, methods_stack=((<DictionaryAnalyzer>, 'к', 375, 0),)),\n",
      "\n",
      "Parse(word='кофе', tag=OpencorporaTag('NOUN,inan,masc,Fixd sing,ablt'), normal_form='кофе', score=0.1875, methods_stack=((<DictionaryAnalyzer>, 'кофе', 170, 4),)),\n",
      "\n",
      "Parse(word='мы', tag=OpencorporaTag('NPRO,1per plur,nomn'), normal_form='мы', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'мы', 1990, 0),)),\n",
      "\n",
      "Parse(word='можем', tag=OpencorporaTag('VERB,impf,intr plur,1per,pres,indc'), normal_form='мочь', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'можем', 1979, 2),)),\n",
      "\n",
      "Parse(word='вычислить', tag=OpencorporaTag('INFN,perf,tran'), normal_form='вычислить', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'вычислить', 617, 0),)),\n",
      "\n",
      "Parse(word='косинусное', tag=OpencorporaTag('ADJF,Qual neut,sing,nomn'), normal_form='косинусный', score=0.3348214285714286, methods_stack=((<DictionaryAnalyzer>, 'синусное', 12, 14), (<UnknownPrefixAnalyzer>, 'ко'))),\n",
      "\n",
      "Parse(word='расстояние', tag=OpencorporaTag('NOUN,inan,neut sing,nomn'), normal_form='расстояние', score=0.714285, methods_stack=((<DictionaryAnalyzer>, 'расстояние', 76, 0),)),\n",
      "\n",
      "Parse(word='между', tag=OpencorporaTag('PREP'), normal_form='между', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'между', 24, 0),)),\n",
      "\n",
      "Parse(word='вектором', tag=OpencorporaTag('NOUN,inan,masc sing,ablt'), normal_form='вектор', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'вектором', 33, 4),)),\n",
      "\n",
      "Parse(word='кофе', tag=OpencorporaTag('NOUN,inan,masc,Fixd sing,ablt'), normal_form='кофе', score=0.1875, methods_stack=((<DictionaryAnalyzer>, 'кофе', 170, 4),)),\n",
      "\n",
      "Parse(word='и', tag=OpencorporaTag('CONJ'), normal_form='и', score=0.997671, methods_stack=((<DictionaryAnalyzer>, 'и', 20, 0),)),\n",
      "\n",
      "Parse(word='векторами', tag=OpencorporaTag('NOUN,inan,masc plur,ablt'), normal_form='вектор', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'векторами', 33, 10),)),\n",
      "\n",
      "Parse(word='других', tag=OpencorporaTag('ADJF,Apro,Subx plur,gent'), normal_form='другой', score=0.647058, methods_stack=((<DictionaryAnalyzer>, 'других', 1362, 21),)),\n",
      "\n",
      "Parse(word='слов', tag=OpencorporaTag('NOUN,inan,neut plur,gent'), normal_form='слово', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'слов', 54, 7),)),\n",
      "\n",
      "Parse(word='в', tag=OpencorporaTag('PREP'), normal_form='в', score=0.999764, methods_stack=((<DictionaryAnalyzer>, 'в', 375, 0),)),\n",
      "\n",
      "Parse(word='модели', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='модель', score=0.545454, methods_stack=((<DictionaryAnalyzer>, 'модели', 13, 1),)),\n",
      "\n",
      "Parse(word='word2vec', tag=OpencorporaTag('LATN'), normal_form='word2vec', score=1.0, methods_stack=((<LatinAnalyzer>, 'Word2Vec'),)),\n",
      "\n",
      "Parse(word='слово', tag=OpencorporaTag('NOUN,inan,neut sing,nomn'), normal_form='слово', score=0.615384, methods_stack=((<DictionaryAnalyzer>, 'слово', 54, 0),)),\n",
      "\n",
      "Parse(word='которое', tag=OpencorporaTag('ADJF,Apro,Subx,Anph neut,sing,nomn'), normal_form='который', score=0.772727, methods_stack=((<DictionaryAnalyzer>, 'которое', 1788, 13),)),\n",
      "\n",
      "Parse(word='имеет', tag=OpencorporaTag('VERB,impf,tran sing,3per,pres,indc'), normal_form='иметь', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'имеет', 1646, 5),)),\n",
      "\n",
      "Parse(word='наименьшее', tag=OpencorporaTag('ADJF,Supr,Qual neut,sing,nomn'), normal_form='наименьший', score=0.666666, methods_stack=((<DictionaryAnalyzer>, 'наименьшее', 508, 14),)),\n",
      "\n",
      "Parse(word='косинусное', tag=OpencorporaTag('ADJF,Qual neut,sing,nomn'), normal_form='косинусный', score=0.3348214285714286, methods_stack=((<DictionaryAnalyzer>, 'синусное', 12, 14), (<UnknownPrefixAnalyzer>, 'ко'))),\n",
      "\n",
      "Parse(word='расстояние', tag=OpencorporaTag('NOUN,inan,neut sing,nomn'), normal_form='расстояние', score=0.714285, methods_stack=((<DictionaryAnalyzer>, 'расстояние', 76, 0),)),\n",
      "\n",
      "Parse(word='будет', tag=OpencorporaTag('VERB,impf,intr sing,3per,futr,indc'), normal_form='быть', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'будет', 587, 12),)),\n",
      "\n",
      "Parse(word='считаться', tag=OpencorporaTag('INFN,impf,intr'), normal_form='считаться', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'считаться', 224, 0),)),\n",
      "\n",
      "Parse(word='наиболее', tag=OpencorporaTag('ADVB'), normal_form='наиболее', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'наиболее', 3, 0),)),\n",
      "\n",
      "Parse(word='похожим', tag=OpencorporaTag('ADJF,Qual masc,sing,ablt'), normal_form='похожий', score=0.3333333333333333, methods_stack=((<DictionaryAnalyzer>, 'похожим', 370, 5),)),\n",
      "\n",
      "Parse(word='на', tag=OpencorporaTag('PREP'), normal_form='на', score=0.99931, methods_stack=((<DictionaryAnalyzer>, 'на', 24, 0),)),\n",
      "\n",
      "Parse(word='кофе', tag=OpencorporaTag('NOUN,inan,masc,Fixd sing,ablt'), normal_form='кофе', score=0.1875, methods_stack=((<DictionaryAnalyzer>, 'кофе', 170, 4),))\n"
     ]
    }
   ],
   "source": [
    "# Создаем список, в который записываются результаты морф анализа\n",
    "dict_tok = list()\n",
    "for tokens in text_tokens:\n",
    "    p = pm.parse(str(tokens))[0]\n",
    "    dict_tok.append(p)\n",
    "\n",
    "#print(lemmas_tok)\n",
    "\n",
    "# Выводим на экран элементы списка с новой строчки и интервалом между ними для удобства чтения\n",
    "print(\",\\n\\n\".join(map(str, dict_tok))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['например', 'если', 'мы', 'хотеть', 'найти', 'наиболее', 'похожий', 'слово', 'к', 'кофе', 'мы', 'мочь', 'вычислить', 'косинусный', 'расстояние', 'между', 'вектор', 'кофе', 'и', 'вектор', 'другой', 'слово', 'в', 'модель', 'word2vec', 'слово', 'который', 'иметь', 'наименьший', 'косинусный', 'расстояние', 'быть', 'считаться', 'наиболее', 'похожий', 'на', 'кофе']\n"
     ]
    }
   ],
   "source": [
    "# Выводим список слов в начальной форме\n",
    "lemmas_tok = list()\n",
    "for tokens in text_tokens:\n",
    "    p = pm.parse(str(tokens))[0].normal_form\n",
    "    lemmas_tok.append(p)\n",
    "\n",
    "print(lemmas_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Достоинства:\n",
    "\n",
    "- умеет составлять разборы, находить лемму, склонять и спрягать\n",
    "- генерирует гипотезы для незнакомых слов\n",
    "- написан полностью на питоне и быстрее, чем Mystem (и есть ускоренная версия с вставками на c++)\n",
    "- может работать с украинским языком (но словари нужно отдельно устанавливать)\n",
    "\n",
    "Недостатки:\n",
    "\n",
    "- качество хуже, чем у Mystem\n",
    "- работает только на уровне отдельных слов и не учитывает контекст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pymystem3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MyStem — это продукт от Яндекса, предоставляющий возможности для морфологического и синтаксического анализа текстов. MyStem использует собственные алгоритмы для определения начальной формы слова и его грамматических характеристик.\n",
    "\n",
    "pymystem3 - это питоновская обертка для MyStem. Если на компьюетере нет Mystem, библиотека его скачает с сервера Яндекса, а потом будет запускать автоматически, а ответ возвращать интерпретатору Python. Сама библиотека работает медленно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymystem3\n",
    "from pymystem3 import Mystem as mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write your text:Write your text:После обучения модели нам нужно понять насколько верно модель определяет комментарии. Мы будем делать это по фактическим контрольным данным. Для этого мы оставим часть датасета для тестирования.\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Write your text:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Write', ' ', 'your', ' ', 'text', ':', 'после', ' ', 'обучение', ' ', 'модель', ' ', 'мы', ' ', 'нужно', ' ', 'понимать', ' ', 'насколько', ' ', 'верно', ' ', 'модель', ' ', 'определять', ' ', 'комментарий', '. ', 'мы', ' ', 'быть', ' ', 'делать', ' ', 'это', ' ', 'по', ' ', 'фактический', ' ', 'контрольный', ' ', 'данные', '. ', 'для', ' ', 'это', ' ', 'мы', ' ', 'оставлять', ' ', 'часть', ' ', 'датасет', ' ', 'для', ' ', 'тестирование', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# Проводим сразу лемматизацию, так как pymystem3 автоматически проводит токенизацию, \n",
    "# но сохраняет пробелы и пунктуацию как элементы списка\n",
    "m = mystem()\n",
    "lemmas = m.lemmatize(text)\n",
    "\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write your text:после обучение модель мы нужно понимать насколько верно модель определять комментарий. мы быть делать это по фактический контрольный данные. для это мы оставлять часть датасет для тестирование.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Можно объединить лемматизированные слова обратно в текст\n",
    "print(''.join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'analysis': [], 'text': 'Write'}, {'text': ' '}, {'analysis': [], 'text': 'your'}, {'text': ' '}, {'analysis': [], 'text': 'text'}, {'text': ':'}, {'analysis': [{'lex': 'после', 'wt': 0.9774551099, 'gr': 'PR='}], 'text': 'После'}, {'text': ' '}, {'analysis': [{'lex': 'обучение', 'wt': 1, 'gr': 'S,сред,неод=(вин,мн|род,ед|им,мн)'}], 'text': 'обучения'}, {'text': ' '}, {'analysis': [{'lex': 'модель', 'wt': 0.9989162385, 'gr': 'S,жен,неод=(пр,ед|вин,мн|дат,ед|род,ед|им,мн)'}], 'text': 'модели'}, {'text': ' '}, {'analysis': [{'lex': 'мы', 'wt': 1, 'gr': 'SPRO,мн,1-л=дат'}], 'text': 'нам'}, {'text': ' '}, {'analysis': [{'lex': 'нужно', 'wt': 0.9310085783, 'gr': 'ADV,прдк='}], 'text': 'нужно'}, {'text': ' '}, {'analysis': [{'lex': 'понимать', 'wt': 1, 'gr': 'V=инф,сов'}], 'text': 'понять'}, {'text': ' '}, {'analysis': [{'lex': 'насколько', 'wt': 1, 'gr': 'ADV='}], 'text': 'насколько'}, {'text': ' '}, {'analysis': [{'lex': 'верно', 'wt': 0.9056031244, 'gr': 'ADV,вводн='}], 'text': 'верно'}, {'text': ' '}, {'analysis': [{'lex': 'модель', 'wt': 0.9996417855, 'gr': 'S,жен,неод=(вин,ед|им,ед)'}], 'text': 'модель'}, {'text': ' '}, {'analysis': [{'lex': 'определять', 'wt': 1, 'gr': 'V,пе=непрош,ед,изъяв,3-л,несов'}], 'text': 'определяет'}, {'text': ' '}, {'analysis': [{'lex': 'комментарий', 'wt': 1, 'gr': 'S,муж,неод=(пр,ед|вин,мн|им,мн)'}], 'text': 'комментарии'}, {'text': '. '}, {'analysis': [{'lex': 'мы', 'wt': 1, 'gr': 'SPRO,мн,1-л=им'}], 'text': 'Мы'}, {'text': ' '}, {'analysis': [{'lex': 'быть', 'wt': 1, 'gr': 'V,нп=(непрош,мн,изъяв,1-л|мн,пов,1-л)'}], 'text': 'будем'}, {'text': ' '}, {'analysis': [{'lex': 'делать', 'wt': 1, 'gr': 'V,несов,пе=инф'}], 'text': 'делать'}, {'text': ' '}, {'analysis': [{'lex': 'это', 'wt': 0.7809833731, 'gr': 'SPRO,ед,сред,неод=(вин|им)'}], 'text': 'это'}, {'text': ' '}, {'analysis': [{'lex': 'по', 'wt': 1, 'gr': 'PR='}], 'text': 'по'}, {'text': ' '}, {'analysis': [{'lex': 'фактический', 'wt': 1, 'gr': 'A=(дат,мн,полн|твор,ед,полн,муж|твор,ед,полн,сред)'}], 'text': 'фактическим'}, {'text': ' '}, {'analysis': [{'lex': 'контрольный', 'wt': 1, 'gr': 'A=(дат,мн,полн|твор,ед,полн,муж|твор,ед,полн,сред)'}], 'text': 'контрольным'}, {'text': ' '}, {'analysis': [{'lex': 'данные', 'wt': 0.9553083191, 'gr': 'S,мн,неод=дат'}], 'text': 'данным'}, {'text': '. '}, {'analysis': [{'lex': 'для', 'wt': 1, 'gr': 'PR='}], 'text': 'Для'}, {'text': ' '}, {'analysis': [{'lex': 'это', 'wt': 0.5385233383, 'gr': 'SPRO,ед,сред,неод=род'}], 'text': 'этого'}, {'text': ' '}, {'analysis': [{'lex': 'мы', 'wt': 1, 'gr': 'SPRO,мн,1-л=им'}], 'text': 'мы'}, {'text': ' '}, {'analysis': [{'lex': 'оставлять', 'wt': 1, 'gr': 'V,пе=(мн,пов,1-л,сов|непрош,мн,изъяв,1-л,сов)'}], 'text': 'оставим'}, {'text': ' '}, {'analysis': [{'lex': 'часть', 'wt': 1, 'gr': 'S,жен,неод=(вин,ед|им,ед)'}], 'text': 'часть'}, {'text': ' '}, {'analysis': [{'lex': 'датасет', 'wt': 0.6742067265, 'qual': 'bastard', 'gr': 'S,муж,неод=род,ед'}], 'text': 'датасета'}, {'text': ' '}, {'analysis': [{'lex': 'для', 'wt': 1, 'gr': 'PR='}], 'text': 'для'}, {'text': ' '}, {'analysis': [{'lex': 'тестирование', 'wt': 1, 'gr': 'S,сред,неод=(вин,мн|род,ед|им,мн)'}], 'text': 'тестирования'}, {'text': '.'}, {'text': '\\n'}]\n"
     ]
    }
   ],
   "source": [
    "# Морфологический анализ (только для русского, другие языки не может)\n",
    "ana = m.analyze(text)\n",
    "print(ana[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analysis': [], 'text': 'Write'}\n",
      "{'text': ' '}\n",
      "{'analysis': [], 'text': 'your'}\n",
      "{'text': ' '}\n",
      "{'analysis': [], 'text': 'text'}\n",
      "{'text': ':'}\n",
      "{'analysis': [{'lex': 'после', 'wt': 0.9774551099, 'gr': 'PR='}], 'text': 'После'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'обучение', 'wt': 1, 'gr': 'S,сред,неод=(вин,мн|род,ед|им,мн)'}], 'text': 'обучения'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'модель', 'wt': 0.9989162385, 'gr': 'S,жен,неод=(пр,ед|вин,мн|дат,ед|род,ед|им,мн)'}], 'text': 'модели'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'мы', 'wt': 1, 'gr': 'SPRO,мн,1-л=дат'}], 'text': 'нам'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'нужно', 'wt': 0.9310085783, 'gr': 'ADV,прдк='}], 'text': 'нужно'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'понимать', 'wt': 1, 'gr': 'V=инф,сов'}], 'text': 'понять'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'насколько', 'wt': 1, 'gr': 'ADV='}], 'text': 'насколько'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'верно', 'wt': 0.9056031244, 'gr': 'ADV,вводн='}], 'text': 'верно'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'модель', 'wt': 0.9996417855, 'gr': 'S,жен,неод=(вин,ед|им,ед)'}], 'text': 'модель'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'определять', 'wt': 1, 'gr': 'V,пе=непрош,ед,изъяв,3-л,несов'}], 'text': 'определяет'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'комментарий', 'wt': 1, 'gr': 'S,муж,неод=(пр,ед|вин,мн|им,мн)'}], 'text': 'комментарии'}\n",
      "{'text': '. '}\n",
      "{'analysis': [{'lex': 'мы', 'wt': 1, 'gr': 'SPRO,мн,1-л=им'}], 'text': 'Мы'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'быть', 'wt': 1, 'gr': 'V,нп=(непрош,мн,изъяв,1-л|мн,пов,1-л)'}], 'text': 'будем'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'делать', 'wt': 1, 'gr': 'V,несов,пе=инф'}], 'text': 'делать'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'это', 'wt': 0.7809833731, 'gr': 'SPRO,ед,сред,неод=(вин|им)'}], 'text': 'это'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'по', 'wt': 1, 'gr': 'PR='}], 'text': 'по'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'фактический', 'wt': 1, 'gr': 'A=(дат,мн,полн|твор,ед,полн,муж|твор,ед,полн,сред)'}], 'text': 'фактическим'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'контрольный', 'wt': 1, 'gr': 'A=(дат,мн,полн|твор,ед,полн,муж|твор,ед,полн,сред)'}], 'text': 'контрольным'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'данные', 'wt': 0.9553083191, 'gr': 'S,мн,неод=дат'}], 'text': 'данным'}\n",
      "{'text': '. '}\n",
      "{'analysis': [{'lex': 'для', 'wt': 1, 'gr': 'PR='}], 'text': 'Для'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'это', 'wt': 0.5385233383, 'gr': 'SPRO,ед,сред,неод=род'}], 'text': 'этого'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'мы', 'wt': 1, 'gr': 'SPRO,мн,1-л=им'}], 'text': 'мы'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'оставлять', 'wt': 1, 'gr': 'V,пе=(мн,пов,1-л,сов|непрош,мн,изъяв,1-л,сов)'}], 'text': 'оставим'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'часть', 'wt': 1, 'gr': 'S,жен,неод=(вин,ед|им,ед)'}], 'text': 'часть'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'датасет', 'wt': 0.6742067265, 'qual': 'bastard', 'gr': 'S,муж,неод=род,ед'}], 'text': 'датасета'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'для', 'wt': 1, 'gr': 'PR='}], 'text': 'для'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'тестирование', 'wt': 1, 'gr': 'S,сред,неод=(вин,мн|род,ед|им,мн)'}], 'text': 'тестирования'}\n",
      "{'text': '.'}\n",
      "{'text': '\\n'}\n"
     ]
    }
   ],
   "source": [
    "# Вывод морф анализа каждого слова с новой строчки\n",
    "for word in ana[:]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Достоинства:\n",
    "\n",
    "- хорошее качество разбора\n",
    "- по умолчанию разрешается частеречная омонимия (внутри части речи остается)\n",
    "- при разборе учитывается контекст\n",
    "- совместим с разметкой НКРЯ\n",
    "\n",
    "Недостатки:\n",
    "\n",
    "- медленный\n",
    "- analyze возвращает неудобный json\n",
    "\n",
    "Текст, очищенный от пунктуации, mystem обрабатывает в разы быстрее! Несколько секунд против десятков минут."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDPipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UDPipe - это готовый пайплайн* для токенизации, частеречной разметки, лемматизации и синтаксической разметки. Работает с файлами в формате CoNLL-U\n",
    "\n",
    "Чтобы работать с UDPipe, нужно выбрать модель: уже готовую, или обучить на своих данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запускала на Colab: https://colab.research.google.com/drive/1mY33bdlqRW7EKQtAhjevkwDtm_KRPPu0?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ufal.udpipe\n",
      "  Downloading ufal.udpipe-1.3.1.1-cp37-cp37m-win_amd64.whl (892 kB)\n",
      "     -------------------------------------- 892.5/892.5 kB 1.8 MB/s eta 0:00:00\n",
      "Installing collected packages: ufal.udpipe\n",
      "Successfully installed ufal.udpipe-1.3.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\slava\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\slava\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\slava\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\slava\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install ufal.udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ufal.udpipe import Model, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "После обучения модели нам нужно понять насколько верно модель определяет комментарии. Мы будем делать это по фактическим контрольным данным. Для этого мы оставим часть датасета для тестирования.\n"
     ]
    }
   ],
   "source": [
    "sent = input ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "UDPIPE_MODEL_FN = \"model_ru.udpipe\"\n",
    "!wget -O {UDPIPE_MODEL_FN} https://github.com/jwijffels/udpipe.models.ud.2.0/blob/master/inst/udpipe-ud-2.0-170801/russian-ud-2.0-170801.udpipe?raw=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model.load(UDPIPE_MODEL_FN) # загружаем модель, сохраняем в переменную model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(model, 'generic_tokenizer', '','','') #функции нужно 5 аргументов,но нам важны только 2\n",
    "#сохраняем в переменную результат токенизации\n",
    "ud_res = []\n",
    "parsed = pipeline.process(sent) # функция process сделает синтаксический анализ, сохраняем еще раз\n",
    "\n",
    "print(parsed) # печатаем результат\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дефолтный теггинг иногда может быть ошибочным: теггер предсказывает морфологические свойства токена по последним четырем символам каждого слова. Он генерирует гипотезы относительно части речи и морфологических тегов этого слова, а затем отбирает лучший вариант."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
