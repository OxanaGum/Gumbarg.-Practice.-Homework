{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer as lemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer as sb\n",
    "from pymystem3 import Mystem as mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write your text:Сегодня кафедра объединяет исследователей в области когнитивной, коммуникативной, компьютерной и экспериментальной лингвистики, а также ученых, работающих в области индоевропеистики и славистики. Во время обучения студенты получают базовые знания в области теории языка, коммуникативных практик, математических методов в лингвистике.\n"
     ]
    }
   ],
   "source": [
    "#Просим пользователя ввсети текст\n",
    "text = input(\"Write your text:\")\n",
    "\n",
    "#words_tokens = word_tokenize(text)\n",
    "\n",
    "#stemmer = sb(\"russian\")\n",
    "#words_lemmas = [stemmer.stem(word) for word in words_tokens]\n",
    "#print(words_lemmas)\n",
    "\n",
    "#stemmer = SnowballStemmer(\"russian\")\n",
    "#text = \"Лемматизированная форма слова листья это лист\"\n",
    "#tokens = word_tokenize(text)\n",
    "#lemmatized_words = [stemmer.stem(word) for word in tokens]\n",
    "#print(lemmatized_words)\n",
    "\n",
    "#for word in words_tokens:\n",
    "#    words_lemmas = lemmatizer.lemmatize(word) \n",
    "#print(words_lemmas)\n",
    "\n",
    "#words_lemmas = mystem.lemmatize(words_tokens)\n",
    "#print(''.join(words_lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write your text:Сегодня кафедра объединяет исследователей в области когнитивной, коммуникативной, компьютерной и экспериментальной лингвистики, а также ученых, работающих в области индоевропеистики и славистики. Во время обучения студенты получают базовые знания в области теории языка, коммуникативных практик, математических методов в лингвистике.\n",
      "['Сегодня', 'кафедра', 'объединяет', 'исследователей', 'в', 'области', 'когнитивной', ',', 'коммуникативной', ',', 'компьютерной', 'и', 'экспериментальной', 'лингвистики', ',', 'а', 'также', 'ученых', ',', 'работающих', 'в', 'области', 'индоевропеистики', 'и', 'славистики', '.', 'Во', 'время', 'обучения', 'студенты', 'получают', 'базовые', 'знания', 'в', 'области', 'теории', 'языка', ',', 'коммуникативных', 'практик', ',', 'математических', 'методов', 'в', 'лингвистике', '.']\n"
     ]
    }
   ],
   "source": [
    "#from nltk.stem import WordNetLemmatizer\n",
    "#from nltk.corpus import wordnet\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "#text = input(\"Write your text:\")\n",
    "#words_tokens = str(word_tokenize(text))\n",
    "#lemmatized_word = lemmatizer.lemmatize(words_tokens)  # Providing the 'word' and 'pos' arguments\n",
    "#print(lemmatized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Сегодня', 'кафедра', 'объединяет', 'исследователей', 'в', 'области', 'когнитивной', 'коммуникативной', 'компьютерной', 'и', 'экспериментальной', 'лингвистики', 'а', 'также', 'ученых', 'работающих', 'в', 'области', 'индоевропеистики', 'и', 'славистики', 'Во', 'время', 'обучения', 'студенты', 'получают', 'базовые', 'знания', 'в', 'области', 'теории', 'языка', 'коммуникативных', 'практик', 'математических', 'методов', 'в', 'лингвистике']\n"
     ]
    }
   ],
   "source": [
    "#Токенизация\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "words_tokens = str(tokenizer.tokenize(text))\n",
    "\n",
    "print(words_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"['\", 'сегодня', \"', '\", 'кафедра', \"', '\", 'объединять', \"', '\", 'исследователь', \"', '\", 'в', \"', '\", 'область', \"', '\", 'когнитивный', \"', '\", 'коммуникативный', \"', '\", 'компьютерный', \"', '\", 'и', \"', '\", 'экспериментальный', \"', '\", 'лингвистика', \"', '\", 'а', \"', '\", 'также', \"', '\", 'ученый', \"', '\", 'работать', \"', '\", 'в', \"', '\", 'область', \"', '\", 'индоевропеистика', \"', '\", 'и', \"', '\", 'славистика', \"', '\", 'во', \"', '\", 'время', \"', '\", 'обучение', \"', '\", 'студент', \"', '\", 'получать', \"', '\", 'базовый', \"', '\", 'знание', \"', '\", 'в', \"', '\", 'область', \"', '\", 'теория', \"', '\", 'язык', \"', '\", 'коммуникативный', \"', '\", 'практика', \"', '\", 'математический', \"', '\", 'метод', \"', '\", 'в', \"', '\", 'лингвистика', \"']\\n\"]\n"
     ]
    }
   ],
   "source": [
    "#Лемматизация\n",
    "m = mystem(disambiguation=False)\n",
    "lemmas = list(m.lemmatize(words_tokens))\n",
    "\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['сегодня', 'кафедра', 'объединять', 'исследователь', 'в', 'область', 'когнитивный', 'коммуникативный', 'компьютерный', 'и', 'экспериментальный', 'лингвистика', 'а', 'также', 'ученый', 'работать', 'в', 'область', 'индоевропеистика', 'и', 'славистика', 'во', 'время', 'обучение', 'студент', 'получать', 'базовый', 'знание', 'в', 'область', 'теория', 'язык', 'коммуникативный', 'практика', 'математический', 'метод', 'в', 'лингвистика']\n"
     ]
    }
   ],
   "source": [
    "#Почистить список от лишних символов\n",
    "for element in lemmas:\n",
    "    if element == \"', '\" or \" [' \": lemmas.remove(element)\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "лингвистика\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "your_word = input()\n",
    "i=0\n",
    "for words in lemmas:\n",
    "    if your_word == words: i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "k=0\n",
    "for words in lemmas:\n",
    "    k += 1\n",
    "print(k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
